<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>code snippets etc. | articles in the "Linear Algebra" category</title>
    <link rel="shortcut icon" type="image/png" href="../favicon.png">
    <link rel="shortcut icon" type="image/x-icon" href="../favicon.ico">
    <link rel="stylesheet" href="../theme/css/main.css" type="text/css" />
    <link rel="stylesheet" href="../theme/css/pygments.css" type="text/css" />
    <meta name="generator" content="Pelican" />
    <meta name="description" content="" />
    <meta name="author" content="" />
</head>
<body>
    <header>
        <nav>
            <ul>

                <li class="ephemeral selected"><a href="../category/linear-algebra.html">Linear Algebra</a></li>
                <li><a href="../">Home</a></li>
                <li><a href="https://github.com/raoulbia">GitHub</a></li>
                <li><a href="../archives">Archives</a></li>
            </ul>
        </nav>
        <div class="header_box">
            <h1><a href="../">code snippets etc.</a></h1>
        </div>
    </header>
    <div id="wrapper">
        <div id="content">            <h4 class="date">Oct 01, 2017</h4>

            <article class="post">
                <h2 class="title">
                    <a href="../linear_algebra.html" rel="bookmark" title="Permanent Link to &quot;Linear Algebra&quot;">Linear Algebra</a>
                </h2>

                
                

                <p>On this page I keep some notes abbout GloVe and word2vec word embeddings. It's work in progress...</p>
<h4>Matrix Factorization</h4>
<ul>
<li>refers to decomposing matrices into their constituent parts</li>
<li>makes it easier to calculate more complex matrix operations</li>
<li>A common analogy for matrix decomposition is the factoring of numbers, such as the factoring of 10 into 2 x 5. 
  For this reason, matrix decomposition is also called matrix factorization. Like factoring real values, there are many 
  ways to decompose a matrix, hence there are a range of different matrix decomposition techniques.</li>
<li>Matrix factorization methods such as Latent Semantic Analysis (LSA) use low-rank approximations to decompose large 
  matrices capturing statistical information about a corpus. 
  (<a href="">source</a>https://blog.acolyer.org/2016/04/22/glove-global-vectors-for-word-representation/)</li>
<li>Common methods: QR, SVD, LU (see Resources below)</li>
</ul>
<h4>Resources</h4>
<ul>
<li><a href="https://machinelearningmastery.com/introduction-to-matrix-decompositions-for-machine-learning/">A Gentle Introduction to Matrix Factorization for Machine Learning</a></li>
<li><a href="https://stepik.org/lesson/8513/step/1?unit=1609">Linear Algebra: Problems and Methods (Stepik)</a></li>
</ul>
                <div class="clear"></div>

                <div class="info">
                    <a href="../linear_algebra.html">posted at 00:00</a>
                    &nbsp;&middot;&nbsp;<a href="../category/linear-algebra.html" rel="tag">Linear Algebra</a>
                </div>
            </article>

            <div class="clear"></div>
            <footer>
                <p>
                <a href="https://github.com/jody-frankowski/blue-penguin">Blue Penguin</a> Theme
                &middot;
                Powered by <a href="http://getpelican.com">Pelican</a>
            </footer>
        </div>
        <div class="clear"></div>
    </div>
</body>
</html>