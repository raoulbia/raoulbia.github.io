<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>code snippets etc. | Page 14</title>
    <link rel="shortcut icon" type="image/png" href="./favicon.png">
    <link rel="shortcut icon" type="image/x-icon" href="./favicon.ico">
    <link rel="stylesheet" href="./theme/css/main.css" type="text/css" />
    <link rel="stylesheet" href="./theme/css/pygments.css" type="text/css" />
    <meta name="generator" content="Pelican" />
    <meta name="description" content="" />
    <meta name="author" content="" />
</head>
<body>
    <header>
        <nav>
            <ul>
                <li><a href="./">Home</a></li>
                <li><a href="https://github.com/raoulbia">GitHub</a></li>
                <li><a href="./archives">Archives</a></li>
            </ul>
        </nav>
        <div class="header_box">
            <h1><a href="./">code snippets etc.</a></h1>
        </div>
    </header>
    <div id="wrapper">
        <div id="content">            <h4 class="date">Nov 23, 2017</h4>

            <article class="post">
                <h2 class="title">
                    <a href="./ksql.html" rel="bookmark" title="Permanent Link to &quot;KSql Cheatsheet&quot;">KSql Cheatsheet</a>
                </h2>

                
                

                <h3>Installation</h3>
<ul>
<li>Download and Setup Java 8 JDK: <code>sudo apt install openjdk-8-jdk</code></li>
<li>wget https://packages.confluent.io/archive/6.2/confluent-6.2.0.tar.gz</li>
<li>tar -xvf confluent-6.2.0</li>
<li><code>sudo ln -s confluent-6.2.0/ confluent</code></li>
<li>add to <code>~/.bashrc</code>:</li>
<li><code>export  export PATH=/opt/confluent/bin:$PATH</code> (or where ever the tar has been unzipped to.</li>
<li><code>export CONFLUENT_HOME=/opt/confluent</code></li>
</ul>
<h3>Start KSql</h3>
<div class="highlight"><pre><span></span>confluent <span class="nb">local</span> services ksql-server start
confluent <span class="nb">local</span> services ksql-server status
</pre></div>


<h4>Topics</h4>
<p>Note: By default KSql will show only new arriving data.</p>
<p>Use <code>FROM BEGINNING</code> to see all existing data AND keep listening for new data.</p>
<p>Also useful for checking that topic has a KEY!!</p>
<ul>
<li><code>print 'topic_name' FROM BEGINNING LIMIT 1;</code></li>
</ul>
<h4>auto.offset.reset</h4>
<p>By default KSql <code>select</code> only shows new incoming data aka <code>current offset</code>. </p>
<p>Must run command below (Only valid for current KSql session! Must re-issue each tme entering KSql prompt).</p>
<div class="highlight"><pre><span></span>SET &#39;auto.offset.reset&#39;=&#39;earliest&#39; ;
</pre></div>


<p>to reset this setting:</p>
<div class="highlight"><pre><span></span>UNSET &#39;auto.offset.reset&#39;;
</pre></div>


<h4>KSql CREATE TABLE (<a href="https://docs.ksqldb.io/en/latest/developer-guide/ksqldb-reference/create-table/">KSQL Syntax Reference</a> )</h4>
<div class="highlight"><pre><span></span><span class="nv">create</span> <span class="nv">table</span> <span class="nv">GTFS_DELAYS_TABLE</span> <span class="ss">(</span><span class="nv">id</span> <span class="nv">VARCHAR</span> <span class="nv">PRMARY</span> <span class="nv">KEY</span>, <span class="nv">delay</span> <span class="nv">INT</span>, <span class="nv">timestamp</span> <span class="nv">INT</span><span class="ss">)</span> <span class="nv">WITH</span> <span class="ss">(</span><span class="nv">KAFKA_TOPIC</span><span class="o">=</span><span class="s1">&#39;</span><span class="s">gtfs-delays</span><span class="s1">&#39;</span>, <span class="nv">VALUE_FORMAT</span><span class="o">=</span><span class="s1">&#39;</span><span class="s">JSON</span><span class="s1">&#39;</span><span class="ss">)</span> <span class="c1">;</span>
<span class="k">show</span> <span class="nv">tables</span> <span class="c1">;</span>
<span class="nv">describe</span> <span class="nv">GTFS_DELAYS_TABLE</span> <span class="c1">;</span>
</pre></div>


<div class="highlight"><pre><span></span>create table GTFS_TABLE <span class="o">(</span> <span class="se">\</span>
id VARCHAR PRIMARY KEY, <span class="se">\</span>
trip_update_trip_trip_id VARCHAR, <span class="se">\</span>
trip_update_trip_start_time VARCHAR, <span class="se">\</span>
trip_update_trip_start_date VARCHAR,<span class="se">\</span>
trip_update_trip_schedule_relationship VARCHAR, <span class="se">\</span>
trip_update_trip_route_id VARCHAR, <span class="se">\</span>
trip_update_stop_time_update_stop_sequence INT, <span class="se">\</span>
trip_update_stop_time_update_departure_delay INT, <span class="se">\</span>
trip_update_stop_time_update_stop_id VARCHAR, <span class="se">\</span>
trip_update_stop_time_update_schedule_relationship VARCHAR, <span class="se">\</span>
trip_update_stop_time_update_arrival_delay INT <span class="se">\</span>
<span class="o">)</span> WITH <span class="o">(</span><span class="nv">KAFKA_TOPIC</span><span class="o">=</span><span class="s1">&#39;GTFS_TOPIC&#39;</span>, <span class="nv">VALUE_FORMAT</span><span class="o">=</span><span class="s1">&#39;JSON&#39;</span><span class="o">)</span> <span class="p">;</span>
</pre></div>


<p>To persist the above table see below. </p>
<div class="highlight"><pre><span></span>create table GTFS_TABLE2 AS select * from GTFS_TABLE ;
</pre></div>


<p>Before terminating a persistent table we must first terminate that query!</p>
<div class="highlight"><pre><span></span><span class="k">show</span> <span class="nv">queries</span> <span class="c1">;</span>
<span class="nv">terminate</span> <span class="o">&lt;</span><span class="nv">query_name</span><span class="o">&gt;</span> <span class="c1">;</span>
<span class="nv">drop</span> <span class="nv">table</span> <span class="o">&lt;</span><span class="nv">table_name</span><span class="o">&gt;</span> <span class="c1">;</span>
</pre></div>


<h4>KSql SELECT</h4>
<div class="highlight"><pre><span></span>select trip_update_trip_trip_id, trip_update_stop_time_update_stop_sequence, trip_update_stop_time_update_departure_delay from GTFS_TABLE emit changes ;
</pre></div>


<div class="highlight"><pre><span></span>select trip_update_trip_trip_id, trip_update_stop_time_update_stop_sequence, sum(trip_update_stop_time_update_departure_delay) from GTFS_TABLE group by trip_update_trip_trip_id, trip_update_stop_time_update_stop_sequence emit changes ;
</pre></div>


<h4>KSql streams</h4>
<p>KSql streams are used to interrogate the underlying data.</p>
<div class="highlight"><pre><span></span><span class="nv">create</span> <span class="nv">stream</span> <span class="nv">users_stream</span> <span class="ss">(</span><span class="nv">name</span> <span class="nv">VARCHAR</span>, <span class="nv">countrycode</span> <span class="nv">VARCHAR</span><span class="ss">)</span> <span class="nv">WITH</span> <span class="ss">(</span><span class="nv">KAFKA_TOPIC</span><span class="o">=</span><span class="s1">&#39;</span><span class="s">USERS</span><span class="s1">&#39;</span>, <span class="nv">VALUE_FORMAT</span><span class="o">=</span><span class="s1">&#39;</span><span class="s">DELIMITED</span><span class="s1">&#39;</span><span class="ss">)</span> <span class="c1">;</span>
<span class="nv">list</span> <span class="nv">streams</span> <span class="c1">;</span>
<span class="nv">select</span> <span class="nv">name</span>, <span class="nv">countrycode</span> <span class="nv">from</span> <span class="nv">users_stream</span> <span class="nv">emit</span> <span class="nv">changes</span><span class="c1">;</span>
<span class="nv">select</span> <span class="nv">name</span>, <span class="nv">countrycode</span> <span class="nv">from</span> <span class="nv">users_stream</span> <span class="nv">emit</span> <span class="nv">changes</span> <span class="nv">limit</span> <span class="mi">5</span><span class="c1">;</span>
<span class="nv">select</span> <span class="nv">countrycode</span>, <span class="nv">count</span><span class="ss">(</span><span class="o">*</span><span class="ss">)</span> <span class="nv">from</span> <span class="nv">users_stream</span> <span class="nv">group</span> <span class="nv">by</span> <span class="nv">countrycode</span><span class="c1">;</span>
<span class="nv">drop</span> <span class="nv">stream</span> <span class="k">if</span> <span class="nv">exists</span> <span class="nv">users_stream</span> <span class="c1">;</span>
<span class="nv">drop</span> <span class="nv">stream</span> <span class="k">if</span> <span class="nv">exists</span> <span class="nv">users_stream</span> <span class="nv">delete</span> <span class="nv">topic</span> <span class="c1">;</span>
</pre></div>


<div class="highlight"><pre><span></span><span class="nt">create</span><span class="w"> </span><span class="nt">stream</span><span class="w"> </span><span class="nt">gtfs_delays_stream</span><span class="w"> </span><span class="o">(</span><span class="nt">id</span><span class="w"> </span><span class="nt">VARCHAR</span><span class="o">,</span><span class="w"> </span><span class="nt">delay</span><span class="w"> </span><span class="nt">INT</span><span class="o">,</span><span class="w"> </span><span class="nt">timestamp</span><span class="w"> </span><span class="nt">INT</span><span class="o">)</span><span class="w"> </span><span class="nt">WITH</span><span class="w"> </span><span class="o">(</span><span class="nt">KAFKA_TOPIC</span><span class="o">=</span><span class="s1">&#39;gtfs-delays&#39;</span><span class="o">,</span><span class="w"> </span><span class="nt">VALUE_FORMAT</span><span class="o">=</span><span class="s1">&#39;JSON&#39;</span><span class="o">)</span><span class="w"> </span><span class="o">;</span><span class="w"></span>
<span class="nt">describe</span><span class="w"> </span><span class="nt">gtfs_delays_stream</span><span class="w"> </span><span class="o">;</span><span class="w"></span>
<span class="nt">select</span><span class="w"> </span><span class="nt">TIMESTAMPTOSTRING</span><span class="o">(</span><span class="nt">rowtime</span><span class="o">,</span><span class="w"> </span><span class="s1">&#39;dd/MMM HH:mm&#39;</span><span class="o">)</span><span class="w"> </span><span class="nt">as</span><span class="w"> </span><span class="nt">createtime</span><span class="o">,</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="nt">from</span><span class="w"> </span><span class="nt">gtfs_delays_stream</span><span class="w"> </span><span class="nt">emit</span><span class="w"> </span><span class="nt">changes</span><span class="w"> </span><span class="nt">limit</span><span class="w"> </span><span class="nt">5</span><span class="o">;</span><span class="w"></span>
</pre></div>


<div class="highlight"><pre><span></span>create stream GTFS_STREAM ( \
id VARCHAR, \
trip_update STRUCT \
    &lt; trip STRUCT \
        &lt; trip_id VARCHAR, start_time VARCHAR, start_date VARCHAR, schedule_relationship VARCHAR, route_id VARCHAR&gt; \
    &gt;,  \
stop_time_update STRUCT &lt; stop_sequence INT, arrival STRUCT &lt; delay INT &gt;, stop_id VARCHAR, schedule_relationship VARCHAR &gt;
) WITH (KAFKA_TOPIC=&#39;GTFS_TOPIC&#39;, VALUE_FORMAT=&#39;JSON&#39;) ;

select * from GTFS_STREAM emit changes ;
</pre></div>


<h3>Other Useful Commands</h3>
<ul>
<li><code>kafka-consumer-groups.sh  --list --bootstrap-server localhost:9092</code></li>
<li><code>zookeeper-shell.sh localhost:2181 ls /brokers/ids</code> to list active brokers. </li>
</ul>
                <div class="clear"></div>

                <div class="info">
                    <a href="./ksql.html">posted at 00:00</a>
                    &nbsp;&middot;&nbsp;<a href="./category/ksql.html" rel="tag">KSql</a>
                </div>
            </article>            <h4 class="date">Nov 23, 2017</h4>

            <article class="post">
                <h2 class="title">
                    <a href="./snowsql.html" rel="bookmark" title="Permanent Link to &quot;SnowSQL Cheatsheet&quot;">SnowSQL Cheatsheet</a>
                </h2>

                
                

                <h4>SnowSQL CLI Config file</h4>
<p>The onfig file is located here: <code>/home/&lt;user&gt;/.snowsql</code>. This is where you configure custom connection parameters.</p>
<p>In your config file you need to modify this line:</p>
<p><code>log_file = ../snowsql_rt.log</code> to this <code>log_file = ~/.snowsql/snowsql_rt.log</code></p>
<h4>Connect to Snowflake account with SnowSQL CLI</h4>
<p><code>snowsql --accountname 123.north-europe.azure --username &lt;username&gt;</code></p>
<p><code>snowsql -c my_connection</code></p>
<h4>Bulk Loading from Local Source</h4>
<p>Create the Employee table</p>
<div class="highlight"><pre><span></span><span class="n">create</span><span class="w"> </span><span class="ow">or</span><span class="w"> </span><span class="n">replace</span><span class="w"> </span><span class="n">table</span><span class="w"> </span><span class="n">Employee</span><span class="w"> </span><span class="p">(</span><span class="w"></span>
<span class="w">    </span><span class="n">row_id</span><span class="w"> </span><span class="n">integer</span><span class="p">,</span><span class="w"></span>
<span class="w">    </span><span class="n">first_name</span><span class="w"> </span><span class="n">varchar</span><span class="p">(</span><span class="mi">50</span><span class="p">),</span><span class="w"></span>
<span class="w">    </span><span class="n">last_name</span><span class="w"> </span><span class="n">varchar</span><span class="p">(</span><span class="mi">50</span><span class="p">),</span><span class="w"></span>
<span class="w">    </span><span class="n">city</span><span class="w"> </span><span class="n">varchar</span><span class="p">(</span><span class="mi">50</span><span class="p">),</span><span class="w"></span>
<span class="w">    </span><span class="n">state</span><span class="w"> </span><span class="n">varchar</span><span class="p">(</span><span class="mi">50</span><span class="p">),</span><span class="w"></span>
<span class="w">    </span><span class="n">hire_date</span><span class="w"> </span><span class="n">date</span><span class="p">,</span><span class="w"></span>
<span class="w">    </span><span class="n">salary</span><span class="w"> </span><span class="n">integer</span><span class="w"></span>
<span class="p">)</span><span class="w"></span>
</pre></div>


<p>Some Employee queries</p>
<div class="highlight"><pre><span></span>select * From Public.Employee
drop table Employee
truncate table Employee
</pre></div>


<p>Create a File Format Object</p>
<div class="highlight"><pre><span></span>create or replace file format employee_csv
  type = &#39;CSV&#39;
  field_delimiter = &#39;,&#39;
  skip_header = 1;
</pre></div>


<p>Create an internal Stage object</p>
<div class="highlight"><pre><span></span>create or replace stage employee_stage
  file_format = employee_csv;
</pre></div>


<p>Display the stages we have created</p>
<div class="highlight"><pre><span></span><span class="k">show</span> <span class="nv">stages</span>
</pre></div>


<p>See what's in the employee_stage</p>
<div class="highlight"><pre><span></span><span class="n">list</span><span class="w"> </span><span class="nv">@employee_stage</span><span class="p">;</span><span class="w"></span>
</pre></div>


<p>Move the data in employee_stage to the employee table. Skip the entire file on any errors</p>
<div class="highlight"><pre><span></span><span class="n">copy</span><span class="w"> </span><span class="k">into</span><span class="w"> </span><span class="n">employee</span><span class="w"></span>
<span class="w">  </span><span class="k">from</span><span class="w"> </span><span class="nv">@employee_stage</span><span class="w"></span>
<span class="w">  </span><span class="n">on_error</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s1">&#39;skip_file&#39;</span><span class="p">;</span><span class="w"></span>
</pre></div>


<p>Move the data in employee_stage to the employee table. Skip just the rows causing any errors</p>
<div class="highlight"><pre><span></span><span class="n">copy</span><span class="w"> </span><span class="k">into</span><span class="w"> </span><span class="n">employee</span><span class="w"></span>
<span class="w">  </span><span class="k">from</span><span class="w"> </span><span class="nv">@employee_stage</span><span class="w"></span>
<span class="w">  </span><span class="n">on_error</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s1">&#39;continue&#39;</span><span class="p">;</span><span class="w"></span>
</pre></div>


<p>Remove file from stage</p>
<div class="highlight"><pre><span></span><span class="n">remove</span><span class="w"> </span><span class="nv">@employee_stage</span><span class="w"> </span><span class="n">pattern</span><span class="o">=</span><span class="s1">&#39;.*.csv.gz&#39;</span><span class="p">;</span><span class="w"></span>
</pre></div>


<p>Create a table to hold all errors during ingestion (VIDEO 21 8:00)</p>
<div class="highlight"><pre><span></span>create or replace table employee_errors as 
select * from table(validate(employee, job_id =&gt; &#39;019b04c4-0c52-f649-0001-3d6e0002d14e&#39;));
select * from employee_errors
</pre></div>


<p>Describe file format</p>
<div class="highlight"><pre><span></span>describe file format employee_csv
</pre></div>


<h4>Storage Integration: Bulk Loading from External Source</h4>
<p>Notes:</p>
<ul>
<li>
<p>to verify STORAGE INTEGRATION made it to Azure, go to Azure AAD &gt; Entreprise applications &gt; you should see Snowflake </p>
</li>
<li>
<p>after creating STORAGE INTEGRATION, must authorize Snowflake in Access Control (IAM)
  go to Storage Account &gt; Access Control (IAM) &gt; Add a role assignment &gt; "Storage Blob Data Contributor"
  click +Select Members and search for Snowflake principal described above &gt; Select &gt; Review+Assign</p>
</li>
</ul>
<div class="highlight"><pre><span></span><span class="k">SHOW</span> <span class="nv">FILE</span> <span class="nv">FORMATS</span>
<span class="k">SHOW</span> <span class="nv">TABLES</span>
<span class="k">SHOW</span> <span class="nv">STAGES</span>

<span class="o">//</span> <span class="nv">CLEAN</span> <span class="nv">UP</span>
<span class="nv">USE</span> <span class="nv">ROLE</span> <span class="nv">ACCOUNTADMIN</span> <span class="c1">;</span>
<span class="nv">DROP</span> <span class="nv">INTEGRATION</span> <span class="k">IF</span> <span class="nv">EXISTS</span> <span class="nv">AZURE_STORAGE</span> <span class="c1">;</span>
<span class="nv">DROP</span> <span class="nv">FILE</span> <span class="nv">FORMAT</span> <span class="k">IF</span> <span class="nv">EXISTS</span> <span class="nv">CSV_FORMAT</span> <span class="c1">;</span>
<span class="nv">DROP</span> <span class="nv">STAGE</span> <span class="k">IF</span> <span class="nv">EXISTS</span> <span class="nv">GTFS</span> <span class="c1">;</span>
</pre></div>


<p>Step 1: Create Table</p>
<div class="highlight"><pre><span></span><span class="n">create</span><span class="w"> </span><span class="ow">or</span><span class="w"> </span><span class="n">replace</span><span class="w"> </span><span class="n">table</span><span class="w"> </span><span class="n">agency</span><span class="p">(</span><span class="w"></span>
<span class="w">  </span><span class="n">agency_id</span><span class="w"> </span><span class="n">varchar</span><span class="p">(</span><span class="mi">50</span><span class="p">),</span><span class="w"> </span>
<span class="w">  </span><span class="n">agency_name</span><span class="w"> </span><span class="n">varchar</span><span class="p">(</span><span class="mi">50</span><span class="p">),</span><span class="w">  </span>
<span class="w">  </span><span class="n">agency_url</span><span class="w"> </span><span class="n">varchar</span><span class="p">(</span><span class="mi">50</span><span class="p">),</span><span class="w"> </span>
<span class="w">  </span><span class="n">agency_timezone</span><span class="w"> </span><span class="n">varchar</span><span class="p">(</span><span class="mi">50</span><span class="p">)</span><span class="w"></span>
<span class="p">);</span><span class="w"></span>
</pre></div>


<p>Step 2: Setup Storage Integration</p>
<div class="highlight"><pre><span></span>USE ROLE ACCOUNTADMIN ;
CREATE OR REPLACE STORAGE INTEGRATION AZURE_STORAGE
  TYPE = EXTERNAL_STAGE
  STORAGE_PROVIDER = AZURE
  ENABLED = TRUE
  AZURE_TENANT_ID = &lt; tenant id &gt;
  STORAGE_ALLOWED_LOCATIONS = (&#39;*&#39;) ;

GRANT USAGE ON INTEGRATION AZURE_STORAGE TO SYSADMIN ;

DESCRIBE STORAGE INTEGRATION AZURE_STORAGE ;

USE ROLE SYSADMIN ; -- De-esclate the role
</pre></div>


<p>Step 3: Setup File Format</p>
<div class="highlight"><pre><span></span>create or replace file format csv_format
  type = &#39;CSV&#39;
  field_delimiter = &#39;,&#39;
  skip_header = 1
  null_if = (&#39;NULL&#39;, &#39;null&#39;)
  empty_field_as_null = TRUE
  compression = gzip ;
</pre></div>


<p>Step 4: Set up Stage</p>
<div class="highlight"><pre><span></span><span class="nt">create</span><span class="w"> </span><span class="nt">or</span><span class="w"> </span><span class="nt">replace</span><span class="w"> </span><span class="nt">stage</span><span class="w"> </span><span class="nt">gtfs</span><span class="w"></span>
<span class="w">  </span><span class="nt">storage_integration</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nt">AZURE_STORAGE</span><span class="w"></span>
<span class="w">  </span><span class="nt">url</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s1">&#39;azure://&lt;container_name&gt;.blob.core.windows.net/gtfs &#39;</span><span class="w"></span>
<span class="w">  </span><span class="nt">file_format</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nt">CSV_FORMAT</span><span class="w"> </span><span class="o">;</span><span class="w"></span>
</pre></div>


<p>Note: it can take an hour or two for Azure to create objects necessary for the integration</p>
<div class="highlight"><pre><span></span><span class="n">list</span><span class="w"> </span><span class="nv">@gtfs</span><span class="w"> </span><span class="p">;</span><span class="w"></span>
</pre></div>
                <div class="clear"></div>

                <div class="info">
                    <a href="./snowsql.html">posted at 00:00</a>
                    &nbsp;&middot;&nbsp;<a href="./category/databases.html" rel="tag">Databases</a>
                </div>
            </article>

                <div class="clear"></div>
                <div class="pages">

                    <a href="./index13.html" class="prev_page">&larr;&nbsp;Previous</a>

                    <a href="./index15.html" class="next_page">Next&nbsp;&rarr;</a>

                    <span>Page 14 of 25</span>
                </div>

            <div class="clear"></div>
            <footer>
                <p>
                <a href="https://github.com/jody-frankowski/blue-penguin">Blue Penguin</a> Theme
                &middot;
                Powered by <a href="http://getpelican.com">Pelican</a>
            </footer>
        </div>
        <div class="clear"></div>
    </div>
</body>
</html>