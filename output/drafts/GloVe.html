<!DOCTYPE html>
<html lang="en">
<head>
          <title>code snippets etc.</title>
        <meta charset="utf-8" />




</head>

<body id="index" class="home">
        <header id="banner" class="body">
                <h1><a href="../">code snippets etc. <strong></strong></a></h1>
        </header><!-- /#banner -->
        <nav id="menu"><ul>
            <li><a href="https://github.com/raoulbia">GitHub</a></li>
        </ul></nav><!-- /#menu -->
<section id="content" class="body">
  <header>
    <h2 class="entry-title">
      <a href="../drafts/GloVe.html" rel="bookmark"
         title="Permalink to GloVe">GloVe</a></h2>
 
  </header>
  <footer class="post-info">
    <abbr class="published" title="2017-09-02T00:00:00+01:00">
      Sat 02 September 2017
    </abbr>
  </footer><!-- /.post-info -->
  <div class="entry-content">
    <h4>Singular Value Decomposition by Gradient Descent</h4>
<p>As an alternative to a full fledged SVD compuation, the matrices $U$ and $V$ can be found using Stochastic Gradient Descent (SGD). This is in many cases preferred because:</p>
<ul>
<li>SVD is slow. </li>
<li>SVD requires care dealing with missing data (<a href="https://www.coursera.org/learn/matrix-factorization/lecture/kVjSo/gradient-descent-techniques">source</a>).</li>
<li>Gradient Descent is much faster and can deal (i.e. ignores) with missing data.</li>
<li>In general, Gradient descent is a way to minimize an objective function $J(\Theta)$. <a href="http://cs231n.github.io/optimization-1/">Click here to learn more about SGD</a>.</li>
</ul>
<p>The key idea behind this approach is to find the best k-rank approximation of the original matrix by searching for the matrices with the least error.</p>
<ul>
<li>Look at the squared error of individual predictions i.e. the predictions' contribution to the sum of squared errors (SSE).</li>
</ul>
<p><strong>Simplified SVD</strong></p>
<ul>
<li>
<p>Decomposition: $$R = B + PQ^T$$</p>
</li>
<li>
<p>Scoring Rule: $$ s(u, i) = b_{ui} + \sum_{f}{p_{uf}q_{if}} $$</p>
</li>
<li>
<p>Caveat: This is no longer a true SVD ($P$ and $Q$ are not orthogonal)</p>
</li>
</ul>
<p><br></p>
<p><strong>FunkSVD</strong> (an alternative approach to SGD)</p>
<ul>
<li>Train features one at a time</li>
<li>Train first feature until convergence</li>
<li>Then train the next</li>
<li>Ignore missing values (mostly)</li>
<li>Learn offset from personalized mean</li>
</ul>
<p><br></p>
<p><strong>Update Step</strong></p>
<p>$$\epsilon_{ui} = r_{ui} - s(u,i)$$
 $$\Delta q_{if} = \lambda(\epsilon_{ui}p_{uf} - \gamma q_{if})$$
 $$\Delta p_{uf} = \lambda(\epsilon_{ui}q_{if} - \gamma p_{uf})$$</p>
<p>where $\lambda$ is the learning rate and $\gamma$ is the regularization (biases against extreme value)</p>
  </div><!-- /.entry-content -->
</section>
        <footer id="contentinfo" class="body">
                <address id="about" class="vcard body">
                Proudly powered by <a href="http://getpelican.com/">Pelican</a>,
                which takes great advantage of <a href="http://python.org">Python</a>.
                </address><!-- /#about -->
        </footer><!-- /#contentinfo -->
</body>
</html>