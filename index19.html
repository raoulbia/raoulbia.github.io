<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>code snippets etc. | Page 19</title>
    <link rel="shortcut icon" type="image/png" href="./favicon.png">
    <link rel="shortcut icon" type="image/x-icon" href="./favicon.ico">
    <link rel="stylesheet" href="./theme/css/main.css" type="text/css" />
    <link rel="stylesheet" href="./theme/css/pygments.css" type="text/css" />
    <link rel="stylesheet" href="./static/custom-code.css" type="text/css" />
    <meta name="generator" content="Pelican" />
    <meta name="description" content="" />
    <meta name="author" content="" />
</head>
<body>
    <header>
        <nav>
            <ul>
                <li><a href="./">Home</a></li>
                <li><a href="./archives">Archives</a></li>
            </ul>
        </nav>
        <div class="header_box">
            <h1><a href="./">code snippets etc.</a></h1>
        </div>
    </header>
    <div id="wrapper">
        <div id="content">            <h4 class="date">Nov 23, 2017</h4>

            <article class="post">
                <h2 class="title">
                    <a href="./ksql.html" rel="bookmark" title="Permanent Link to &quot;KSql Cheatsheet&quot;">KSql Cheatsheet</a>
                </h2>

                
                

                <h3>Installation</h3>
<ul>
<li>Download and Setup Java 8 JDK: <code>sudo apt install openjdk-8-jdk</code></li>
<li>wget https://packages.confluent.io/archive/6.2/confluent-6.2.0.tar.gz</li>
<li>tar -xvf confluent-6.2.0</li>
<li><code>sudo ln -s confluent-6.2.0/ confluent</code></li>
<li>add to <code>~/.bashrc</code>:</li>
<li><code>export  export PATH=/opt/confluent/bin:$PATH</code> (or where ever the tar has been unzipped to.</li>
<li><code>export CONFLUENT_HOME=/opt/confluent</code></li>
</ul>
<h3>Start KSql</h3>
<div class="highlight"><pre><span></span><code>confluent<span class="w"> </span><span class="nb">local</span><span class="w"> </span>services<span class="w"> </span>ksql-server<span class="w"> </span>start
confluent<span class="w"> </span><span class="nb">local</span><span class="w"> </span>services<span class="w"> </span>ksql-server<span class="w"> </span>status
</code></pre></div>

<h4>Topics</h4>
<p>Note: By default KSql will show only new arriving data.</p>
<p>Use <code>FROM BEGINNING</code> to see all existing data AND keep listening for new data.</p>
<p>Also useful for checking that topic has a KEY!!</p>
<ul>
<li><code>print 'topic_name' FROM BEGINNING LIMIT 1;</code></li>
</ul>
<h4>auto.offset.reset</h4>
<p>By default KSql <code>select</code> only shows new incoming data aka <code>current offset</code>. </p>
<p>Must run command below (Only valid for current KSql session! Must re-issue each tme entering KSql prompt).</p>
<div class="highlight"><pre><span></span><code>SET &#39;auto.offset.reset&#39;=&#39;earliest&#39; ;
</code></pre></div>

<p>to reset this setting:</p>
<div class="highlight"><pre><span></span><code>UNSET &#39;auto.offset.reset&#39;;
</code></pre></div>

<h4>KSql CREATE TABLE (<a href="https://docs.ksqldb.io/en/latest/developer-guide/ksqldb-reference/create-table/">KSQL Syntax Reference</a> )</h4>
<div class="highlight"><pre><span></span><code>create table GTFS_DELAYS_TABLE (id VARCHAR PRMARY KEY, delay INT, timestamp INT) WITH (KAFKA_TOPIC=&#39;gtfs-delays&#39;, VALUE_FORMAT=&#39;JSON&#39;) ;
show tables ;
describe GTFS_DELAYS_TABLE ;
</code></pre></div>

<div class="highlight"><pre><span></span><code>create<span class="w"> </span>table<span class="w"> </span>GTFS_TABLE<span class="w"> </span><span class="o">(</span><span class="w"> </span><span class="se">\</span>
id<span class="w"> </span>VARCHAR<span class="w"> </span>PRIMARY<span class="w"> </span>KEY,<span class="w"> </span><span class="se">\</span>
trip_update_trip_trip_id<span class="w"> </span>VARCHAR,<span class="w"> </span><span class="se">\</span>
trip_update_trip_start_time<span class="w"> </span>VARCHAR,<span class="w"> </span><span class="se">\</span>
trip_update_trip_start_date<span class="w"> </span>VARCHAR,<span class="se">\</span>
trip_update_trip_schedule_relationship<span class="w"> </span>VARCHAR,<span class="w"> </span><span class="se">\</span>
trip_update_trip_route_id<span class="w"> </span>VARCHAR,<span class="w"> </span><span class="se">\</span>
trip_update_stop_time_update_stop_sequence<span class="w"> </span>INT,<span class="w"> </span><span class="se">\</span>
trip_update_stop_time_update_departure_delay<span class="w"> </span>INT,<span class="w"> </span><span class="se">\</span>
trip_update_stop_time_update_stop_id<span class="w"> </span>VARCHAR,<span class="w"> </span><span class="se">\</span>
trip_update_stop_time_update_schedule_relationship<span class="w"> </span>VARCHAR,<span class="w"> </span><span class="se">\</span>
trip_update_stop_time_update_arrival_delay<span class="w"> </span>INT<span class="w"> </span><span class="se">\</span>
<span class="o">)</span><span class="w"> </span>WITH<span class="w"> </span><span class="o">(</span><span class="nv">KAFKA_TOPIC</span><span class="o">=</span><span class="s1">&#39;GTFS_TOPIC&#39;</span>,<span class="w"> </span><span class="nv">VALUE_FORMAT</span><span class="o">=</span><span class="s1">&#39;JSON&#39;</span><span class="o">)</span><span class="w"> </span><span class="p">;</span>
</code></pre></div>

<p>To persist the above table see below. </p>
<div class="highlight"><pre><span></span><code>create table GTFS_TABLE2 AS select * from GTFS_TABLE ;
</code></pre></div>

<p>Before terminating a persistent table we must first terminate that query!</p>
<div class="highlight"><pre><span></span><code>show queries ;
terminate &lt;query_name&gt; ;
drop table &lt;table_name&gt; ;
</code></pre></div>

<h4>KSql SELECT</h4>
<div class="highlight"><pre><span></span><code>select trip_update_trip_trip_id, trip_update_stop_time_update_stop_sequence, trip_update_stop_time_update_departure_delay from GTFS_TABLE emit changes ;
</code></pre></div>

<div class="highlight"><pre><span></span><code>select trip_update_trip_trip_id, trip_update_stop_time_update_stop_sequence, sum(trip_update_stop_time_update_departure_delay) from GTFS_TABLE group by trip_update_trip_trip_id, trip_update_stop_time_update_stop_sequence emit changes ;
</code></pre></div>

<h4>KSql streams</h4>
<p>KSql streams are used to interrogate the underlying data.</p>
<div class="highlight"><pre><span></span><code>create stream users_stream (name VARCHAR, countrycode VARCHAR) WITH (KAFKA_TOPIC=&#39;USERS&#39;, VALUE_FORMAT=&#39;DELIMITED&#39;) ;
list streams ;
select name, countrycode from users_stream emit changes;
select name, countrycode from users_stream emit changes limit 5;
select countrycode, count(*) from users_stream group by countrycode;
drop stream if exists users_stream ;
drop stream if exists users_stream delete topic ;
</code></pre></div>

<div class="highlight"><pre><span></span><code><span class="nt">create</span><span class="w"> </span><span class="nt">stream</span><span class="w"> </span><span class="nt">gtfs_delays_stream</span><span class="w"> </span><span class="o">(</span><span class="nt">id</span><span class="w"> </span><span class="nt">VARCHAR</span><span class="o">,</span><span class="w"> </span><span class="nt">delay</span><span class="w"> </span><span class="nt">INT</span><span class="o">,</span><span class="w"> </span><span class="nt">timestamp</span><span class="w"> </span><span class="nt">INT</span><span class="o">)</span><span class="w"> </span><span class="nt">WITH</span><span class="w"> </span><span class="o">(</span><span class="nt">KAFKA_TOPIC</span><span class="o">=</span><span class="s1">&#39;gtfs-delays&#39;</span><span class="o">,</span><span class="w"> </span><span class="nt">VALUE_FORMAT</span><span class="o">=</span><span class="s1">&#39;JSON&#39;</span><span class="o">)</span><span class="w"> </span><span class="o">;</span>
<span class="nt">describe</span><span class="w"> </span><span class="nt">gtfs_delays_stream</span><span class="w"> </span><span class="o">;</span>
<span class="nt">select</span><span class="w"> </span><span class="nt">TIMESTAMPTOSTRING</span><span class="o">(</span><span class="nt">rowtime</span><span class="o">,</span><span class="w"> </span><span class="s1">&#39;dd/MMM HH:mm&#39;</span><span class="o">)</span><span class="w"> </span><span class="nt">as</span><span class="w"> </span><span class="nt">createtime</span><span class="o">,</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="nt">from</span><span class="w"> </span><span class="nt">gtfs_delays_stream</span><span class="w"> </span><span class="nt">emit</span><span class="w"> </span><span class="nt">changes</span><span class="w"> </span><span class="nt">limit</span><span class="w"> </span><span class="nt">5</span><span class="o">;</span>
</code></pre></div>

<div class="highlight"><pre><span></span><code>create stream GTFS_STREAM ( \
id VARCHAR, \
trip_update STRUCT \
    &lt; trip STRUCT \
        &lt; trip_id VARCHAR, start_time VARCHAR, start_date VARCHAR, schedule_relationship VARCHAR, route_id VARCHAR&gt; \
    &gt;,  \
stop_time_update STRUCT &lt; stop_sequence INT, arrival STRUCT &lt; delay INT &gt;, stop_id VARCHAR, schedule_relationship VARCHAR &gt;
) WITH (KAFKA_TOPIC=&#39;GTFS_TOPIC&#39;, VALUE_FORMAT=&#39;JSON&#39;) ;

select * from GTFS_STREAM emit changes ;
</code></pre></div>

<h3>Other Useful Commands</h3>
<ul>
<li><code>kafka-consumer-groups.sh  --list --bootstrap-server localhost:9092</code></li>
<li><code>zookeeper-shell.sh localhost:2181 ls /brokers/ids</code> to list active brokers. </li>
</ul>
                <div class="clear"></div>

                <div class="info">
                    <a href="./ksql.html">posted at 00:00</a>
                    &nbsp;&middot;&nbsp;<a href="./category/ksql.html" rel="tag">KSql</a>
                </div>
            </article>            <h4 class="date">Nov 23, 2017</h4>

            <article class="post">
                <h2 class="title">
                    <a href="./snowsql.html" rel="bookmark" title="Permanent Link to &quot;SnowSQL Cheatsheet&quot;">SnowSQL Cheatsheet</a>
                </h2>

                
                

                <h4>SnowSQL CLI Config file</h4>
<p>The onfig file is located here: <code>/home/&lt;user&gt;/.snowsql</code>. This is where you configure custom connection parameters.</p>
<p>In your config file you need to modify this line:</p>
<p><code>log_file = ../snowsql_rt.log</code> to this <code>log_file = ~/.snowsql/snowsql_rt.log</code></p>
<h4>Connect to Snowflake account with SnowSQL CLI</h4>
<p><code>snowsql --accountname 123.north-europe.azure --username &lt;username&gt;</code></p>
<p><code>snowsql -c my_connection</code></p>
<h4>Bulk Loading from Local Source</h4>
<p>Create the Employee table</p>
<div class="highlight"><pre><span></span><code><span class="n">create</span><span class="w"> </span><span class="ow">or</span><span class="w"> </span><span class="n">replace</span><span class="w"> </span><span class="n">table</span><span class="w"> </span><span class="n">Employee</span><span class="w"> </span><span class="p">(</span>
<span class="w">    </span><span class="n">row_id</span><span class="w"> </span><span class="n">integer</span><span class="p">,</span>
<span class="w">    </span><span class="n">first_name</span><span class="w"> </span><span class="n">varchar</span><span class="p">(</span><span class="mi">50</span><span class="p">),</span>
<span class="w">    </span><span class="n">last_name</span><span class="w"> </span><span class="n">varchar</span><span class="p">(</span><span class="mi">50</span><span class="p">),</span>
<span class="w">    </span><span class="n">city</span><span class="w"> </span><span class="n">varchar</span><span class="p">(</span><span class="mi">50</span><span class="p">),</span>
<span class="w">    </span><span class="n">state</span><span class="w"> </span><span class="n">varchar</span><span class="p">(</span><span class="mi">50</span><span class="p">),</span>
<span class="w">    </span><span class="n">hire_date</span><span class="w"> </span><span class="n">date</span><span class="p">,</span>
<span class="w">    </span><span class="n">salary</span><span class="w"> </span><span class="n">integer</span>
<span class="p">)</span>
</code></pre></div>

<p>Some Employee queries</p>
<div class="highlight"><pre><span></span><code>select * From Public.Employee
drop table Employee
truncate table Employee
</code></pre></div>

<p>Create a File Format Object</p>
<div class="highlight"><pre><span></span><code><span class="nx">create</span><span class="w"> </span><span class="k">or</span><span class="w"> </span><span class="nx">replace</span><span class="w"> </span><span class="nx">file</span><span class="w"> </span><span class="nx">format</span><span class="w"> </span><span class="nx">employee_csv</span>
<span class="w">  </span><span class="k">type</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="err">&#39;</span><span class="nx">CSV</span><span class="err">&#39;</span>
<span class="w">  </span><span class="nx">field_delimiter</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="sc">&#39;,&#39;</span>
<span class="w">  </span><span class="nx">skip_header</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="mi">1</span><span class="p">;</span>
</code></pre></div>

<p>Create an internal Stage object</p>
<div class="highlight"><pre><span></span><code>create or replace stage employee_stage
  file_format = employee_csv;
</code></pre></div>

<p>Display the stages we have created</p>
<div class="highlight"><pre><span></span><code><span class="k">show</span><span class="w"> </span><span class="nv">stages</span>
</code></pre></div>

<p>See what's in the employee_stage</p>
<div class="highlight"><pre><span></span><code><span class="n">list</span><span class="w"> </span><span class="nv">@employee_stage</span><span class="p">;</span>
</code></pre></div>

<p>Move the data in employee_stage to the employee table. Skip the entire file on any errors</p>
<div class="highlight"><pre><span></span><code><span class="n">copy</span><span class="w"> </span><span class="k">into</span><span class="w"> </span><span class="n">employee</span>
<span class="w">  </span><span class="k">from</span><span class="w"> </span><span class="nv">@employee_stage</span>
<span class="w">  </span><span class="n">on_error</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s1">&#39;skip_file&#39;</span><span class="p">;</span>
</code></pre></div>

<p>Move the data in employee_stage to the employee table. Skip just the rows causing any errors</p>
<div class="highlight"><pre><span></span><code><span class="n">copy</span><span class="w"> </span><span class="k">into</span><span class="w"> </span><span class="n">employee</span>
<span class="w">  </span><span class="k">from</span><span class="w"> </span><span class="nv">@employee_stage</span>
<span class="w">  </span><span class="n">on_error</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s1">&#39;continue&#39;</span><span class="p">;</span>
</code></pre></div>

<p>Remove file from stage</p>
<div class="highlight"><pre><span></span><code><span class="n">remove</span><span class="w"> </span><span class="nv">@employee_stage</span><span class="w"> </span><span class="n">pattern</span><span class="o">=</span><span class="s1">&#39;.*.csv.gz&#39;</span><span class="p">;</span>
</code></pre></div>

<p>Create a table to hold all errors during ingestion (VIDEO 21 8:00)</p>
<div class="highlight"><pre><span></span><code>create or replace table employee_errors as 
select <span class="gs">* from table(validate(employee, job_id =&gt; &#39;019b04c4-0c52-f649-0001-3d6e0002d14e&#39;));</span>
<span class="gs">select *</span> from employee_errors
</code></pre></div>

<p>Describe file format</p>
<div class="highlight"><pre><span></span><code>describe file format employee_csv
</code></pre></div>

<h4>Storage Integration: Bulk Loading from External Source</h4>
<p>Notes:</p>
<ul>
<li>
<p>to verify STORAGE INTEGRATION made it to Azure, go to Azure AAD &gt; Entreprise applications &gt; you should see Snowflake </p>
</li>
<li>
<p>after creating STORAGE INTEGRATION, must authorize Snowflake in Access Control (IAM)
  go to Storage Account &gt; Access Control (IAM) &gt; Add a role assignment &gt; "Storage Blob Data Contributor"
  click +Select Members and search for Snowflake principal described above &gt; Select &gt; Review+Assign</p>
</li>
</ul>
<div class="highlight"><pre><span></span><code>SHOW FILE FORMATS
SHOW TABLES
SHOW STAGES

// CLEAN UP
USE ROLE ACCOUNTADMIN ;
DROP INTEGRATION IF EXISTS AZURE_STORAGE ;
DROP FILE FORMAT IF EXISTS CSV_FORMAT ;
DROP STAGE IF EXISTS GTFS ;
</code></pre></div>

<p>Step 1: Create Table</p>
<div class="highlight"><pre><span></span><code><span class="n">create</span><span class="w"> </span><span class="ow">or</span><span class="w"> </span><span class="n">replace</span><span class="w"> </span><span class="n">table</span><span class="w"> </span><span class="n">agency</span><span class="p">(</span>
<span class="w">  </span><span class="n">agency_id</span><span class="w"> </span><span class="n">varchar</span><span class="p">(</span><span class="mi">50</span><span class="p">),</span><span class="w"> </span>
<span class="w">  </span><span class="n">agency_name</span><span class="w"> </span><span class="n">varchar</span><span class="p">(</span><span class="mi">50</span><span class="p">),</span><span class="w">  </span>
<span class="w">  </span><span class="n">agency_url</span><span class="w"> </span><span class="n">varchar</span><span class="p">(</span><span class="mi">50</span><span class="p">),</span><span class="w"> </span>
<span class="w">  </span><span class="n">agency_timezone</span><span class="w"> </span><span class="n">varchar</span><span class="p">(</span><span class="mi">50</span><span class="p">)</span>
<span class="p">);</span>
</code></pre></div>

<p>Step 2: Setup Storage Integration</p>
<div class="highlight"><pre><span></span><code>USE ROLE ACCOUNTADMIN ;
CREATE OR REPLACE STORAGE INTEGRATION AZURE_STORAGE
  TYPE = EXTERNAL_STAGE
  STORAGE_PROVIDER = AZURE
  ENABLED = TRUE
  AZURE_TENANT_ID = &lt; tenant id &gt;
  STORAGE_ALLOWED_LOCATIONS = (&#39;*&#39;) ;

GRANT USAGE ON INTEGRATION AZURE_STORAGE TO SYSADMIN ;

DESCRIBE STORAGE INTEGRATION AZURE_STORAGE ;

USE ROLE SYSADMIN ; -- De-esclate the role
</code></pre></div>

<p>Step 3: Setup File Format</p>
<div class="highlight"><pre><span></span><code><span class="nx">create</span><span class="w"> </span><span class="k">or</span><span class="w"> </span><span class="nx">replace</span><span class="w"> </span><span class="nx">file</span><span class="w"> </span><span class="nx">format</span><span class="w"> </span><span class="nx">csv_format</span>
<span class="w">  </span><span class="k">type</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="err">&#39;</span><span class="nx">CSV</span><span class="err">&#39;</span>
<span class="w">  </span><span class="nx">field_delimiter</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="sc">&#39;,&#39;</span>
<span class="w">  </span><span class="nx">skip_header</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="mi">1</span>
<span class="w">  </span><span class="nx">null_if</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="p">(</span><span class="err">&#39;</span><span class="nx">NULL</span><span class="err">&#39;</span><span class="p">,</span><span class="w"> </span><span class="err">&#39;</span><span class="nx">null</span><span class="err">&#39;</span><span class="p">)</span>
<span class="w">  </span><span class="nx">empty_field_as_null</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="nx">TRUE</span>
<span class="w">  </span><span class="nx">compression</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="nx">gzip</span><span class="w"> </span><span class="p">;</span>
</code></pre></div>

<p>Step 4: Set up Stage</p>
<div class="highlight"><pre><span></span><code><span class="nt">create</span><span class="w"> </span><span class="nt">or</span><span class="w"> </span><span class="nt">replace</span><span class="w"> </span><span class="nt">stage</span><span class="w"> </span><span class="nt">gtfs</span>
<span class="w">  </span><span class="nt">storage_integration</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nt">AZURE_STORAGE</span>
<span class="w">  </span><span class="nt">url</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s1">&#39;azure://&lt;container_name&gt;.blob.core.windows.net/gtfs &#39;</span>
<span class="w">  </span><span class="nt">file_format</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nt">CSV_FORMAT</span><span class="w"> </span><span class="o">;</span>
</code></pre></div>

<p>Note: it can take an hour or two for Azure to create objects necessary for the integration</p>
<div class="highlight"><pre><span></span><code><span class="n">list</span><span class="w"> </span><span class="nv">@gtfs</span><span class="w"> </span><span class="p">;</span>
</code></pre></div>
                <div class="clear"></div>

                <div class="info">
                    <a href="./snowsql.html">posted at 00:00</a>
                    &nbsp;&middot;&nbsp;<a href="./category/databases.html" rel="tag">Databases</a>
                </div>
            </article>

                <div class="clear"></div>
                <div class="pages">

                    <a href="./index18.html" class="prev_page">&larr;&nbsp;Previous</a>

                    <a href="./index20.html" class="next_page">Next&nbsp;&rarr;</a>

                    <span>Page 19 of 31</span>
                </div>

            <div class="clear"></div>
            <footer>
                <p>
                <a href="https://github.com/jody-frankowski/blue-penguin">Blue Penguin</a> Theme
                &middot;
                Powered by <a href="http://getpelican.com">Pelican</a>
            </footer>
        </div>
        <div class="clear"></div>
    </div>
<script>
// Add copy buttons to all code blocks
document.addEventListener('DOMContentLoaded', function() {
    document.querySelectorAll('pre').forEach(function(pre) {
        // Avoid duplicate buttons
        if (pre.querySelector('.copy-btn')) return;
        var btn = document.createElement('button');
        btn.className = 'copy-btn';
        btn.type = 'button';
        btn.innerText = 'Copy';
        btn.onclick = function() {
            var code = pre.querySelector('code');
            if (code) {
                var text = code.innerText;
                navigator.clipboard.writeText(text).then(function() {
                    btn.innerText = 'Copied!';
                    setTimeout(function() { btn.innerText = 'Copy'; }, 1200);
                });
            }
        };
        pre.appendChild(btn);
        pre.style.position = 'relative';
    });
});
</script>
</body>
</html>