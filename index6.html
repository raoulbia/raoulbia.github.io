<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>code snippets etc. | Page 6</title>
    <link rel="shortcut icon" type="image/png" href="./favicon.png">
    <link rel="shortcut icon" type="image/x-icon" href="./favicon.ico">
    <link rel="stylesheet" href="./theme/css/main.css" type="text/css" />
    <link rel="stylesheet" href="./theme/css/pygments.css" type="text/css" />
    <meta name="generator" content="Pelican" />
    <meta name="description" content="" />
    <meta name="author" content="" />
</head>
<body>
    <header>
        <nav>
            <ul>
                <li><a href="./">Home</a></li>
                <li><a href="./archives">Archives</a></li>
            </ul>
        </nav>
        <div class="header_box">
            <h1><a href="./">code snippets etc.</a></h1>
        </div>
    </header>
    <div id="wrapper">
        <div id="content">            <h4 class="date">Dec 24, 2022</h4>

            <article class="post">
                <h2 class="title">
                    <a href="./azure.html" rel="bookmark" title="Permanent Link to &quot;Azure&quot;">Azure</a>
                </h2>

                
                

                <p><br></p>
<h3>Using Key Vault Secrets in Azure Data Factory</h3>
<p>Key Vault secrets can be used to securely manage connections to services such as NAS drives or Databricks.</p>
<h4>NAS Drive Linked Service</h4>
<ul>
<li>Create a Service Account with priviliges to access a NAS drive</li>
<li>create a new secret in KV with the password of the above Service Account</li>
<li>use the secret, and the Service Account user name, in ADF when configuring a linked service for NAS drive</li>
<li>use the linked service in an ADF Copy data activity</li>
</ul>
<h4>Databricks Linked Service</h4>
<ul>
<li>Generate a token in Databricks and take note of it</li>
<li>create a new secret in KV with the above token</li>
<li>use the secret in ADF when configuring a linked service for databricks</li>
<li>use the linked service in an ADF Databricks notebook activity</li>
</ul>
<h3>Azure ARM</h3>
<h4>Deploy via Powershell</h4>
<ul>
<li><code>New-AzResourceGroup RG01 "South Central US"</code></li>
<li><code>New-AzResourceGroupDeployment</code> adds a deployment to an existing resource group. (https://learn.microsoft.com/en-us/powershell/module/az.resources/new-azresourcegroupdeployment?view=azps-9.3.0)</li>
</ul>
<h4>Useful Links</h4>
<ul>
<li>https://samcogan.com/deploying-resource-groups-with-arm-templates/</li>
</ul>
<p><br></p>
<h3>Azure Fundamentals Exam (AZ-900)</h3>
<ul>
<li><a href="./drafts/azure_cloud_benefits_and_categories.html">Azure Cloud: Benefits and Categories</a></li>
<li><a href="./drafts/azure_architectural_components.html">Azure Architectural Components</a></li>
<li><a href="./drafts/azure_services.html">Azure Services</a></li>
<li><a href="./drafts/azure_solutions.html">Azure Solutions</a></li>
<li><a href="./drafts/azure_management_tools.html">Azure Management Tools</a></li>
<li><a href="./drafts/azure_general_security.html">Azure General Security Features</a></li>
<li><a href="./drafts/azure_network_security.html">Azure Network Security</a></li>
<li><a href="./drafts/azure_identity_governance_privacy_compliance.html">Azure Identity, Governance, Privacy and Compliance Features</a></li>
<li><a href="./drafts/azure_cost_management_and_sla.html">Azure Cost Management and SLA</a></li>
</ul>
<h3>Note about creating a Databricks Resource in Azure Portal</h3>
<ul>
<li>When creating a Databricks resource, Databricks will itself create a separate resource group (RG) with a storage account. This RG cannot be deleted via the porta;. See here for more: https://stackoverflow.com/questions/60694149/unable-to-remove-azure-databricks-managed-resource-group</li>
</ul>
                <div class="clear"></div>

                <div class="info">
                    <a href="./azure.html">posted at 00:00</a>
                    &nbsp;&middot;&nbsp;<a href="./category/azure.html" rel="tag">Azure</a>
                </div>
            </article>            <h4 class="date">Dec 24, 2022</h4>

            <article class="post">
                <h2 class="title">
                    <a href="./azure_data_factory.html" rel="bookmark" title="Permanent Link to &quot;Azure Data Factory&quot;">Azure Data Factory</a>
                </h2>

                
                

                <p><br></p>
<h3>Data Ingress</h3>
<ul>
<li>
<p>Excel File with one or more Sheets </p>
<ul>
<li>New Dataset &gt; File tab &gt; File System &gt; Excel &gt; select linked service to NAS drive &gt; browse to NAS drive file &gt; select a Sheet name if applicable</li>
</ul>
</li>
<li>
<p>Netezza Table</p>
<ul>
<li>New Dataset &gt; Database tab &gt; Netezza &gt; select linked service to Netezza &gt; select table</li>
</ul>
</li>
</ul>
<h3>Linked Services</h3>
<h4>Create a Linked Service to connect ADF to Databricks using Key Vault</h4>
<p>Pre-requisites: </p>
<ul>
<li>
<p>Create a <strong>Key Vault</strong> resource</p>
<ul>
<li><em>Secrets</em> &gt; add a secret holding the Databricks PAT </li>
<li><strong><em>Access policies</em></strong><ul>
<li>click <em>Create</em></li>
<li>select <em>Get</em> and <em>List</em> for <em>Key permissions</em> and <em>Secret permissions</em></li>
<li>in the search box enter the name of your Azure Data Factory, select it when it shows up and click next and create all the way</li>
</ul>
</li>
</ul>
</li>
<li>
<p>This requires two linked services</p>
<ul>
<li>Databrick ls &gt; get it from 'Compute' tab</li>
<li>within this ls you are asked to specify an AKV linked service &gt; click on 'New'</li>
</ul>
</li>
<li>
<p>AKV linked Service</p>
<ul>
<li>Base URL should be prefilled</li>
<li>Authentication Method: <em>System Assigned Managed Identity</em></li>
</ul>
</li>
</ul>
<p><br></p>
<h3>Parameterizing ADF</h3>
<h4>Pass a custom filename to Copy Activity Sink Dataset</h4>
<p>Copy Activity:     import an Excel to Blob Storage as Parquet file</p>
<p>Problem to solve:  give the parquet file a custom name</p>
<p>Solution:          add a parameter to the dataset of interest (source, sink or both depending on use case)</p>
<p>Explanation:       when a copy activity source, or sink, uses a dataset that has a parameter, the parameter will appear in the source, or sink, <em>Dataset properties</em>. Here you can then specifiy a value, either as hard coded string or as yet another parameter, that will then be passed to the Dataset <em>Connection</em> tab where it is referenced and will be replaced by the value passed to it.</p>
<p>Steps: 
* for the sink part (Blob Storage), you need a Parquet dataset with the required Linked Service to ADLS Gen2
* in the Dataset <em>Parameters</em> tab, add a parameter named <em>sink_file</em> (String)
* in the Dataset <em>Connection</em> tab, it is assumed the <em>File Path</em> boxes 1 and 2 are populated with the path to the blob storage container where the files will be saved. For <em>File Path</em> box 3, enter <em>@dataset().sink_file</em>
* in the Copy Activity, under <em>Dataset Properties</em> of the sink, enter the value you wish to pass to the sink dataset. THis can be a string or anotehr parameter e.g. a value captured from a <em>Get Metadata</em> activity, or from a <em>Lookup</em> activity.</p>
<h4>Query Database Tables and Save as Parquet File to Blob Storage</h4>
<p>The solution has two core components: A <em>Lookup</em> activity and a <em>ForEach</em> activity.</p>
<p>The <em>Lookup</em> activity 
* the source dataset can be a CSV file on NAS drive. The columns in the file are <em>table_name</em> and <em>where_filter</em>. Each row represents one table to be queried by the <em>ForEach</em> loop.</p>
<p>The <em>ForEach</em> activity
* consists of two <em>Set variable</em> activities connected to a <em>Copy Data</em> activity.
* the <em>Set variable</em> activities reference the values for table name and for the where filter passed through by the <em>Lookup</em> activity. The syntax is shown in the screenshot below.
* the source of the <em>Copy Data</em> activity uses the following query: <em>@concat('select * from ', item().table_name,' ', item().where_filter)</em>. 
* in the sink of the <em>Copy Data</em> activity we pass a dynamic value to parameter <em>parFile</em> of the sink Parquet dataset: <em>@concat(item().table_name,'_NTZ.parquet')</em></p>
<p><img alt="" src="img/ForEach_activity_all.PNG"></p>
<p>The dynamic filename passed to the Parquet dataset is shown below:</p>
<p><img alt="" src="img/ds_parquet_params_config.PNG"></p>
<h4>Create dynamic folder name for COPY activity (egress)</h4>
<ul>
<li>
<p>source ADLSGen2:<br>
<code>@concat('@concat('custom_dataset/shared_team_data/CG_BPT/BPT_CLIENT_EXCELS/', formatdatetime(utcnow(),'yyyy-MM-dd'))</code></p>
</li>
<li>
<p>sink NAS Drive: <br>
<code>@concat('Azure/CG/static_output_files/', formatDateTime(utcNow(),'yyyy'), '-' ,formatDateTime(utcNow(),'MM'), '-', formatDateTime(utcNow(),'dd'))</code></p>
</li>
</ul>
                <div class="clear"></div>

                <div class="info">
                    <a href="./azure_data_factory.html">posted at 00:00</a>
                    &nbsp;&middot;&nbsp;<a href="./category/azure-cloud.html" rel="tag">Azure Cloud</a>
                </div>
            </article>

                <div class="clear"></div>
                <div class="pages">

                    <a href="./index5.html" class="prev_page">&larr;&nbsp;Previous</a>

                    <a href="./index7.html" class="next_page">Next&nbsp;&rarr;</a>

                    <span>Page 6 of 31</span>
                </div>

            <div class="clear"></div>
            <footer>
                <p>
                <a href="https://github.com/jody-frankowski/blue-penguin">Blue Penguin</a> Theme
                &middot;
                Powered by <a href="http://getpelican.com">Pelican</a>
            </footer>
        </div>
        <div class="clear"></div>
    </div>
</body>
</html>