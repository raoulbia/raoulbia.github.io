<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>code snippets etc. | Page 25</title>
    <link rel="shortcut icon" type="image/png" href="./favicon.png">
    <link rel="shortcut icon" type="image/x-icon" href="./favicon.ico">
    <link rel="stylesheet" href="./theme/css/main.css" type="text/css" />
    <link rel="stylesheet" href="./theme/css/pygments.css" type="text/css" />
    <meta name="generator" content="Pelican" />
    <meta name="description" content="" />
    <meta name="author" content="" />
</head>
<body>
    <header>
        <nav>
            <ul>
                <li><a href="./">Home</a></li>
                <li><a href="./archives">Archives</a></li>
            </ul>
        </nav>
        <div class="header_box">
            <h1><a href="./">code snippets etc.</a></h1>
        </div>
    </header>
    <div id="wrapper">
        <div id="content">            <h4 class="date">Sep 03, 2017</h4>

            <article class="post">
                <h2 class="title">
                    <a href="./Logistic Regression.html" rel="bookmark" title="Permanent Link to &quot;Logistic Regression&quot;">Logistic Regression</a>
                </h2>

                
                

                <p>On this page I keep some ontes on logistic regression.</p>
<h4>Supervised Learning: Regression Techniques</h4>
<p>Most supervised learning problems can be framed formally 
in terms of predicting a response, but prediction alone is often not
the main goal of the analysis. </p>
<p>For example, many applications of
linear regression in the sciences are aimed primarily
at <strong>understanding how the inputs of a system drive the outputs</strong>.</p>
<p>An extremely complicated "black box" giving pure predictions would not be
very useful in and of itself.</p>
<p>(<a href="https://lagunita.stanford.edu/courses/HumanitiesSciences/StatLearning/Winter2016/about">source</a>)</p>
<p><em>Gradient Descent</em>: iterative computation of adjusted feature weights</p>
<h4>Linear vs. Logistic Regression: normalizing</h4>
<ul>
<li>
<p>linear regression: must normalize features</p>
</li>
<li>
<p>logistic regression: no need to normalize the data. </p>
</li>
</ul>
<p>see <a href="https://stats.stackexchange.com/questions/29781/when-conducting-multiple-regression-when-should-you-center-your-predictor-varia">here</a> 
for a good discussion on the topic of data <em>centering</em> and <em>scaling</em> data features.</p>
                <div class="clear"></div>

                <div class="info">
                    <a href="./Logistic Regression.html">posted at 00:00</a>
                    &nbsp;&middot;&nbsp;<a href="./category/logistic-regression.html" rel="tag">Logistic Regression</a>
                </div>
            </article>            <h4 class="date">Sep 01, 2017</h4>

            <article class="post">
                <h2 class="title">
                    <a href="./embeddings.html" rel="bookmark" title="Permanent Link to &quot;Embeddings&quot;">Embeddings</a>
                </h2>

                
                

                <p>On this page I keep some notes abbout <a href="./drafts/GloVe.html">GloVe</a>
and word2vec word embeddings. </p>
<h4>various notes</h4>
<ul>
<li>
<p>Word embeddings are based on the <strong>distributional hypothesis</strong>:</p>
<p>A word’s meaning is encoded in the 
  frequencies with which other words 
  occur near it in natural text.</p>
</li>
</ul>
<p>In other words, a word’s environment encodes a great deal of semantic information. 
  For example, we can infer that <em>oculist</em> and <em>eye-doctor</em> are synonyms from the fact that they are found 
  near the same words in a given corpus.  </p>
<ul>
<li>Algorithms like singular value decomposition (SVD) or principle components analysis (PCA)
  can be used to compress an <code>n × m</code> feature matrix M to an <code>n × k</code> matrix M' (where k &lt;&lt; m) 
  in such a way that M' retains most of the information of M. </li>
</ul>
<p><a href="https://universalflowuniversity.com/Books/Computer%20Programming/Data%20Mining%20and%20Data%20Science/The%20Data%20Science%20Design%20Manual.pdf">source</a></p>
<ul>
<li>
<p>Word representations can be obtained using </p>
</li>
<li>
<p>co-occurrence counts (derived directly from corpus counts</p>
</li>
<li>using association measures such as Point-wise Mutual Information (PMI)</li>
<li>lower-rank representations such as PPMI-SVD and GloVe (<a href="https://arxiv.org/pdf/1606.00819.pdf">source</a>) </li>
<li>
<p>predictive methods e.g. skip-gram (word2vec)</p>
</li>
<li>
<p>A co-occurrence matrix by itself provides word vectors which can in principle be used to compute 
  similarities between words. These vectors are how ever high-dimensional and sparse. </p>
</li>
<li>
<p>There are two approaches to converting high-dimensional and sparse word vectors to low-dimensional and 
  dense vectors:</p>
</li>
<li>
<p>dimensionality reduction e.g. SVD</p>
</li>
<li>
<p>direct learning e.g. word2vec, GloVe</p>
</li>
<li>
<p>For each unique word, word embedding methods aim to find a vector that summarizes its environment 
  and hence reveals its semantics. </p>
</li>
<li>
<p>Algebraic relationships between word vectors correspond to semantic relationships between words.<br />
  For example, it is typical to find that </p>
</li>
</ul>
<p><code>f(France) ≈ f(England) + f(Paris) − f(London)</code></p>
<p>i.e. “France is to England as Paris is to London”. 
  (<a href="https://uwaterloo.ca/scholar/sites/ca.scholar/files/pa2forsy/files/project_dec_3.pdf">source</a>) </p>
<ul>
<li>You can check how good a word embedding is by finding the top 10 nearest neighbors of a word.</li>
</ul>
<h4>word2vec</h4>
<ul>
<li>
<p>word2vec consist of two functions:</p>
</li>
<li>
<p><em>f</em>, which maps words to a word vector</p>
</li>
<li><em>g</em>, which, given a word’s vector, produces an estimate of the probability of finding each other 
    word in vocabulary $V$ near that word in the corpus.</li>
</ul>
<p>where 
   * the values of a given word vector are the parameters we determine when learning g.
   * <em>f</em> and <em>g</em> are trained by stochastic gradient descent so that each predicted word is probable, given
     its predictor word</p>
<h4>GloVe</h4>
<ul>
<li>
<p>GloVe produces embeddings by factorizing the logarithm of the corpus word co-occurrence matrix.</p>
</li>
<li>
<p><em>We tokenize and lowercase each corpus with the Stanford tokenizer, build a vocabulary of the 400,000 most frequent 
 words, and then construct a matrix of co-occurence counts… we use a decreasing weighting function so that word pairs 
 that are d words apart contribute 1/d to the total count.</em></p>
</li>
<li>
<p><code>#TODO</code> look up 1st order vs. 2nd order co-occurrences &gt; see Coursera video</p>
</li>
<li>
<p>While this produces embeddings which are similar to <a href="https://code.google.com/p/word2vec/">word2vec</a>
(which has a great python implementation in <a href="http://radimrehurek.com/gensim/models/word2vec.html">gensim</a>), 
the method is different!</p>
</li>
<li>
<p>co-occurrence: underlying assumption that the meaning of each word can be captured through its co-occurrence pattern 
with context words</p>
</li>
</ul>
<h4>Glossary</h4>
<ul>
<li>
<p>skip-bigram: count how many times a pair of words occurs together in sentences irrespective of their 
  positions in sentences.</p>
</li>
<li>
<p>Word vector dimension: Actually the word vector dimension does not reflect the vocabulary size. 
  What Word2Vec is doing is mapping the words to their representation in a vector space and you can 
  make this space of any dimension you want. 
  <a href="https://stackoverflow.com/questions/38137551/what-is-word-vector-dimension">source</a></p>
</li>
</ul>
<h4>Resources</h4>
<p><strong>Resources covering both word2vec and Glove</strong></p>
<ul>
<li><a href="https://www.csie.ntu.edu.tw/~yvchen/f105-adl/doc/161013_WordVector2.pdf">Applied Deep Learning</a>(slides)</li>
<li><a href="https://uwaterloo.ca/scholar/sites/ca.scholar/files/pa2forsy/files/project_dec_3.pdf">Word vectors and the distributional hypothesis</a>
(focus is on word2vec; good description of the underlying theory)</li>
<li><a href="https://arxiv.org/pdf/1606.00819.pdf">Matrix Factorization using Window Sampling and Negative Sampling for Improved Word Representations</a></li>
<li><a href="https://towardsdatascience.com/word-embeddings-exploration-explanation-and-exploitation-with-code-in-python-5dac99d5d795">Word embeddings: exploration, explanation, and exploitation</a>
  (uses GloVe via Gensim)  </li>
<li><a href="https://machinelearningmastery.com/develop-word-embeddings-python-gensim/">How to Develop Word Embeddings in Python with Gensim</a>
  (discusses pre-trained embeddings)</li>
<li><a href="https://becominghuman.ai/word-embeddings-with-gensim-68e6322afdca">Word Embeddings with Gensim</a>
  (discusses pre-trained embeddings)</li>
<li><a href="https://textminingonline.com/getting-started-with-word2vec-and-glove-in-python">Getting Started with Word2Vec and GloVe in Python</a>
  (uses Gensim)</li>
</ul>
<p><strong>Resources covering word2vec</strong></p>
<ul>
<li><a href="https://datawarrior.wordpress.com/2016/10/12/short-text-categorization-using-deep-neural-networks-and-word-embedding-models/">Short Text Categorization using Deep Neural Networks and Word-Embedding Models</a> 
  (uses Gensim) </li>
<li><a href="https://towardsdatascience.com/learn-word2vec-by-implementing-it-in-tensorflow-45641adaf2ac">Learn Word2Vec by implementing it in tensorflow</a>
  (discusses query word embedding vectors)</li>
<li><a href="https://medium.com/@shubhamagarwal328/playing-around-with-word2vec-natural-language-processing-ccd10a044b1">Playing around with Word2Vec</a>
  (discusses vector similarity, k-means clustering of similar words, uses Gensim)</li>
<li><a href="https://rare-technologies.com/making-sense-of-word2vec/">Making sense of word2vec</a></li>
<li><a href="">Word Embeddings in Python with Spacy and Gensim</a>https://www.shanelynn.ie/word-embeddings-in-python-with-spacy-and-gensim/)</li>
<li><a href="https://recurrentnull.wordpress.com/2016/10/21/sentiment-analysis-of-movie-reviews-2-word2vec/">Sentiment Analysis of Movie Reviews: word2vec</a></li>
<li><a href="http://adventuresinmachinelearning.com/word2vec-tutorial-tensorflow/">word2vec tutorial tensorflow</a></li>
</ul>
<p><strong>Resources covering Glove</strong></p>
<ul>
<li><a href="http://www-nlp.stanford.edu/projects/glove/">Glove Stanford NLP</a></li>
<li>(video) <a href="https://www.youtube.com/watch?v=Y8gKX5zMRyQ">GloVe - Python for Word Representation - PyCon APAC 2018</a></li>
<li><a href="https://medium.com/ai-society/jkljlj-7d6e699895c4">A Comprehensive Introduction to Word Vector Representations</a>
  (discusses SVD)</li>
<li><a href="https://blog.acolyer.org/2016/04/22/glove-global-vectors-for-word-representation/">GloVe: Global Vectors for Word Representation</a>
  (discusses Matrix factorization vs shallow-window methods)</li>
<li><a href="https://medium.com/@japneet121/word-vectorization-using-glove-76919685ee0b">Word vectorization using Glove</a>
  (print the embeddings for the word “samsung”)</li>
<li><a href="https://iksinc.online/tag/co-occurrence-matrix/">How to Use Words Co-Occurrence Statistics to Map Words to Vectors</a> 
  (with a vectors example <a href="https://iksinc.online/2015/06/23/how-to-use-words-co-occurrence-statistics-to-map-words-to-vectors/#comment-395">here</a>)</li>
<li><a href="https://towardsdatascience.com/emnlp-what-is-glove-part-i-3b6ce6a7f970">What is GloVe?</a></li>
</ul>
<h4>Related</h4>
<ul>
<li>
<p>General Overview of Word Embedding techniques</p>
<ul>
<li><a href="https://www.gavagai.se/blog/2015/09/30/a-brief-history-of-word-embeddings/">A Brief History of Word Embeddings (and some clarifications)</a> 
  (includes links to frameworks)</li>
<li><a href="https://freecontent.manning.com/deep-learning-for-text/">Deep Learning for Text</a></li>
<li><a href="https://nlpforhackers.io/word-embeddings/">Complete Guide to Word Embeddings</a> 
  (mentions t-SNE)</li>
<li><a href="http://blog.aylien.com/overview-word-embeddings-history-word2vec-cbow-glove/">An overview of word embeddings and their connection to distributional semantic models</a></li>
<li><a href="https://jjallaire.github.io/deep-learning-with-r-notebooks/notebooks/6.1-using-word-embeddings.nb.html">Using word embeddings</a></li>
<li><a href="https://arxiv.org/ftp/arxiv/papers/1708/1708.03994.pdf">Data Sets: Word Embeddings Learned from Tweets and General Data</a></li>
<li><a href="">Learning Word Vectors for Sentiment Analysis</a>http://ai.stanford.edu/~ang/papers/acl11-WordVectorsSentimentAnalysis.pdf)</li>
<li><a href="https://multithreaded.stitchfix.com/blog/2017/10/18/stop-using-word2vec/">Stop Using word2vec</a></li>
<li><a href="https://radimrehurek.com/gensim/models/keyedvectors.html">What can I do with word vectors?</a> (scroll down)</li>
</ul>
</li>
<li>
<p>Misc. </p>
<ul>
<li><a href="https://stackoverflow.com/questions/37793118/load-pretrained-glove-vectors-in-python">Load Pretrained glove vectors in python</a></li>
<li><a href="https://github.com/OpenNMT/OpenNMT-py/issues/210">How to use pretrained embeddings from Glove?</a>
  (discusses initialisation of word vector when using pre-trained word vecs)</li>
<li><a href="http://text2vec.org/glove-cli.html">text2vec R package</a></li>
<li><a href="http://dsnotes.com/post/glove-enwiki/">GloVe vs word2vec revisited</a>
  (uses text2vec R package)</li>
<li><a href="https://dh2017.adho.org/abstracts/582/582.pdf">Word Vectors in the Eighteenth Century</a></li>
</ul>
</li>
<li>
<p>Books</p>
<ul>
<li><a href="https://www.safaribooksonline.com/library/view/practical-machine-learning/9781491915707/ch04.html">Co-occurrence and Recommendation</a>
  (Practical Machine Learning: Innovations in Recommendation Chapter 4)</li>
<li><a href="http://www.charuaggarwal.net/Text-Learning.pdf">Machine Learning for Text</a>
  (discusses Glove gradient descent optimization in Chapter 10.4)</li>
</ul>
</li>
<li>
<p>Papers</p>
<ul>
<li><a href="https://web.stanford.edu/class/cs224n/reports/2758144.pdf">Extending the Scope of Co-occurrence Embedding</a></li>
<li><a href="http://www.aclweb.org/anthology/Q15-1016">Improving Distributional Similarity with Lessons Learned from Word Embeddings</a></li>
<li><a href="https://arxiv.org/pdf/1602.02215.pdf">Swivel: Improving Embeddings by Noticing What’s Missing</a>
  (good description of optimization pseudocode)</li>
</ul>
</li>
<li>
<p>Algorithms</p>
<ul>
<li><a href="http://ruder.io/optimizing-gradient-descent/index.html#challenges">An overview of gradient descent optimization algorithms</a></li>
<li><a href="https://am207.github.io/2017/wiki/gradientdescent.html#stochastic-gradient-descent">Stochastic gradient descent</a></li>
</ul>
</li>
<li>
<p>Slides</p>
<ul>
<li><a href="https://pdfs.semanticscholar.org/presentation/548c/0412f63ab096c89892d663c34f36da678c10.pdf">Simple   Word    Vector  representa2ons: word2vec,   GloVe</a></li>
<li><a href="http://www.cs.virginia.edu/~kc2wc/teaching/NLP16/slides/07-WordEmbedding.pdf">Word Embeddings</a></li>
<li><a href="https://cs224d.stanford.edu/lectures/CS224d-Lecture3.pdf">Richard Socher slides</a></li>
</ul>
</li>
<li>
<p>Videos</p>
<ul>
<li><a href="https://www.coursera.org/lecture/language-processing/distributional-semantics-bee-and-honey-vs-bee-an-bumblebee-PtRav">Coursera NLP</a></li>
<li><a href="https://www.coursera.org/lecture/nlp-sequence-models/glove-word-vectors-IxDTG">Coursera NLP (Andrew Ng)</a></li>
<li><a href="https://www.coursera.org/lecture/intro-to-deep-learning/word-embeddings-dhzl5">Coursera Intro to Deep Learning</a>
  (word2vec)</li>
<li><a href="http://u.cs.biu.ac.il/~yogo/cvsc2015.pdf">word embeddings: what, how and whither</a></li>
</ul>
</li>
</ul>
<p><em>Code Repos</em></p>
<div class="highlight"><pre><span></span><span class="o">*</span><span class="w"> </span><span class="p">[</span><span class="n">Gensim</span><span class="p">](</span><span class="n">https</span><span class="o">:</span><span class="c1">//radimrehurek.com/gensim/models/word2vec.html)</span>
<span class="w">  </span><span class="p">(</span><span class="n">includes</span><span class="w"> </span><span class="n">word2vec</span><span class="w"> </span><span class="n">and</span><span class="w"> </span><span class="n">GloVe</span><span class="p">)</span>
<span class="o">*</span><span class="w"> </span><span class="p">[</span><span class="n">glove</span><span class="o">-</span><span class="n">python</span><span class="p">](</span><span class="n">https</span><span class="o">:</span><span class="c1">//github.com/maciejkula/glove-python)</span>
<span class="w">  </span><span class="p">(</span><span class="n">maciejkula</span><span class="p">)</span>
<span class="o">*</span><span class="w"> </span><span class="p">[</span><span class="n">tf</span><span class="o">-</span><span class="n">glove</span><span class="p">](</span><span class="n">https</span><span class="o">:</span><span class="c1">//github.com/eschorn0/tf-glove)</span>
<span class="w">  </span><span class="p">(</span><span class="n">Glove</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">Python</span><span class="p">,</span><span class="w"> </span><span class="n">Cython</span><span class="w"> </span><span class="n">and</span><span class="w"> </span><span class="n">Tensorflow</span><span class="w"> </span><span class="n">r1</span><span class="mf">.0</span><span class="w"> </span><span class="n">on</span><span class="w"> </span><span class="n">Ubuntu</span><span class="p">)</span>
<span class="o">*</span><span class="w"> </span><span class="p">[</span><span class="n">glovepy</span><span class="p">](</span><span class="n">https</span><span class="o">:</span><span class="c1">//pypi.org/project/glovepy/)</span>
<span class="w">  </span><span class="p">(</span><span class="n">A</span><span class="w"> </span><span class="n">Python</span><span class="w"> </span><span class="n">implementation</span><span class="w"> </span><span class="n">with</span><span class="w"> </span><span class="n">Cython</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">GloVe</span><span class="p">)</span>
<span class="o">*</span><span class="w"> </span><span class="p">[</span><span class="n">A</span><span class="w"> </span><span class="n">GloVe</span><span class="w"> </span><span class="n">implementation</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="n">Python</span><span class="p">](</span><span class="n">http</span><span class="o">:</span><span class="c1">//www.foldl.me/2014/glove-python/) (foldl)</span>
<span class="o">*</span><span class="w"> </span><span class="p">[</span><span class="n">machine_learning_examples</span><span class="p">](</span><span class="n">https</span><span class="o">:</span><span class="c1">//github.com/lazyprogrammer/machine_learning_examples/tree/master/nlp_class2)</span>
<span class="w">  </span><span class="p">(</span><span class="n">lazyprogrammer</span><span class="p">)</span>
<span class="o">*</span><span class="w"> </span><span class="p">[</span><span class="n">movie</span><span class="w"> </span><span class="n">sentiment</span><span class="p">](</span><span class="n">https</span><span class="o">:</span><span class="c1">//github.com/saurabhmathur96/movie-sentiment)</span>
</pre></div>


<ul>
<li>
<p>Datasets</p>
<ul>
<li><a href="https://github.com/3Top/word2vec-api#where-to-get-a-pretrained-models">Where to get pretrained models</a></li>
</ul>
</li>
</ul>
                <div class="clear"></div>

                <div class="info">
                    <a href="./embeddings.html">posted at 00:00</a>
                    &nbsp;&middot;&nbsp;<a href="./category/embeddings.html" rel="tag">Embeddings</a>
                </div>
            </article>

                <div class="clear"></div>
                <div class="pages">

                    <a href="./index24.html" class="prev_page">&larr;&nbsp;Previous</a>

                    <a href="./index26.html" class="next_page">Next&nbsp;&rarr;</a>

                    <span>Page 25 of 31</span>
                </div>

            <div class="clear"></div>
            <footer>
                <p>
                <a href="https://github.com/jody-frankowski/blue-penguin">Blue Penguin</a> Theme
                &middot;
                Powered by <a href="http://getpelican.com">Pelican</a>
            </footer>
        </div>
        <div class="clear"></div>
    </div>
</body>
</html>